
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{LogisticRegression}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Logistic Regression}\label{logistic-regression}

Logistic Regression is a popular machine learning algorithm used for
classification problems. Classification is just like regression but here
we predict some small number of discrete values for the output instead
of continuous real values. Some examples being predicting whether an
email is spam or not, classifying whether an image contains a cat or a
dog, diagnosing if a person is suffering from a particular disease or
not etc. For the purpose of this assignment we will be working on a
Binary Classification problem where the output y can only take 2 values,
0 and 1.For instance, if we are trying to build a spam classifier for
email, then \(x^{(i)}\) may be some features of a piece of email, and y
may be 1 if it is a piece of spam mail, and 0 otherwise. 0 is also
called the negative class, and 1 the positive class.

Logistic regression algorithm works by predicting the probability of the
positive class given the input data. For doing so it uses sigmoid
function which maps the input in the range 0 to 1. Lets see in detail
how that works.

\begin{align}
z &= \theta_{1}x_{1} + \theta_{2}x_{2} + ... + \theta_{n}x_{n} + b\\
  &= \theta^{T}x + b\\
P(y=1 | x) &= \frac{1}{1 + e^{-z}}\\
where\; x \in \Re^n
\end{align}

\(1/(1 + exp^{-z})\) denotes the sigmoid function and its graph looks
like: \includegraphics{attachment:320px-Logistic-curve.svg.png}

We take a linear combination of the features of a data point with an
added bias (this is called an affine transformation) and then apply
sigmoid function to the resulting value to get the output in range
(0,1). Lets see why it works by considering the geometric
interpertation:
\includegraphics{attachment:Untitled\%20presentation\%20\%286\%29.png}

We have our data points arranged in a 2 dimensional space with half of
the points been labeled by the purple class and the others by yellow.
Lets take the purple points as the positive class (y=1) and yellow
points as negative class (y=0).
\(\theta_{1}x_{1} + \theta_{2}x_{2} + b = 0\) is a line separating
purple and yellow points. As you can see from the figure that means the
purple points lie on the right side of the line
(\(\theta_{1}x_{1} + \theta_{2}x_{2} + b > 0\)) and yellow points lie on
the left (\(\theta_{1}x_{1} + \theta_{2}x_{2} + b < 0\))

As we can see from the graph of the sigmoid function, at 0 it becomes
0.5 and approaches 1 and 0 at +inf and -inf respectively. Hence
sigmoid(\$\theta\emph{\{1\}x}\{1\} + \theta\emph{\{2\}x}\{2\} + b
\() will be 0.5 for the points liying on the line, > 0.5 for the points lying on the right side and < 0.5 for the ponts of the left. Hence we classify points with sigmoid(\)z\$)
\textgreater{} 0.5 belonging to the positive class and \textless{} 0.5
as belonging to the negative class. For more details on logistic
regression check the videos on this playlist
https://www.youtube.com/watch?v=-la3q9d7AKQ\&list=PLNeKWBMsAzboR8vvhnlanxCNr2V7ITuxy

We will be working with breast\_cancer dataset which is available in
sklearn package. The target values are binary (either 0 or 1)
corresponding to whether the tumor is malignant or benign. So this
problem can be modelled as binary classification problem. We will use
logistic regression which is a method used for binary classification to
correctly classify the labels of the class. The dataset has 30 features
but for simplicity we will consider only the first two features

Go through the following link to learn more about the dataset

https://scikit-learn.org/stable/datasets/index.html

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{}importing relevant libraries }
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k}{import} \PY{n}{load\PYZus{}breast\PYZus{}cancer}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{42}\PY{p}{)}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}


    Now we will load the dataset and split it into training and testing sets
respectively. We train our logistic regression classifier using the
training set and evaluate its performance using the test set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Dont change code of this block}
        \PY{n}{datapoints} \PY{o}{=} \PY{n}{load\PYZus{}breast\PYZus{}cancer}\PY{p}{(}\PY{p}{)}
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{datapoints}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{datapoints}\PY{o}{.}\PY{n}{target}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{datapoints}\PY{o}{.}\PY{n}{target}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
        \PY{n}{y\PYZus{}train}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{y\PYZus{}test}\PY{o}{=}\PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    Q1) Can you tell why we use reshape on y\_test and y\_train ? (bonus)

    YOUR ANSWER HERE

    \subsection{Task 1: Select first 2 features of the
data.}\label{task-1-select-first-2-features-of-the-data.}

For the purpose of simplicity and ease of visualization we will be
working only with the first 2 features of our dataset. Implement the
modify function below which takes a numpy 2d array as input with number
of rows being the number of datapoints and columns being the features,
and selects just the first 2 features (or columns) for all the data
points.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{}The function will modify X\PYZus{}train by keeping only the first two features }
        \PY{k}{def} \PY{n+nf}{modify}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{l+s+sd}{    Inputs:}
        \PY{l+s+sd}{        \PYZhy{}X : Data matrix containing m data points each having n features. Shape [m, n]}
        \PY{l+s+sd}{    Outputs:}
        \PY{l+s+sd}{        Modified data matrix containing just the first 2 features of the data points. Shape [m, 2]}
        \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
            
            \PY{k}{return}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} SAMPLE TEST CASE}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Running sample test case}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{a} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}
         
         \PY{n}{ans} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mf}{0.5488135} \PY{p}{,} \PY{l+m+mf}{0.71518937}\PY{p}{]}\PY{p}{,}
                \PY{p}{[}\PY{l+m+mf}{0.4236548} \PY{p}{,} \PY{l+m+mf}{0.64589411}\PY{p}{]}\PY{p}{,}
                \PY{p}{[}\PY{l+m+mf}{0.96366276}\PY{p}{,} \PY{l+m+mf}{0.38344152}\PY{p}{]}\PY{p}{,}
                \PY{p}{[}\PY{l+m+mf}{0.56804456}\PY{p}{,} \PY{l+m+mf}{0.92559664}\PY{p}{]}\PY{p}{,}
                \PY{p}{[}\PY{l+m+mf}{0.0202184} \PY{p}{,} \PY{l+m+mf}{0.83261985}\PY{p}{]}\PY{p}{,}
                \PY{p}{[}\PY{l+m+mf}{0.97861834}\PY{p}{,} \PY{l+m+mf}{0.79915856}\PY{p}{]}\PY{p}{,}
                \PY{p}{[}\PY{l+m+mf}{0.11827443}\PY{p}{,} \PY{l+m+mf}{0.63992102}\PY{p}{]}\PY{p}{,}
                \PY{p}{[}\PY{l+m+mf}{0.52184832}\PY{p}{,} \PY{l+m+mf}{0.41466194}\PY{p}{]}\PY{p}{,}
                \PY{p}{[}\PY{l+m+mf}{0.45615033}\PY{p}{,} \PY{l+m+mf}{0.56843395}\PY{p}{]}\PY{p}{,}
                \PY{p}{[}\PY{l+m+mf}{0.61209572}\PY{p}{,} \PY{l+m+mf}{0.616934}  \PY{p}{]}\PY{p}{]}\PY{p}{)}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{ans}\PY{p}{,} \PY{n}{modify}\PY{p}{(}\PY{n}{a}\PY{p}{)}\PY{p}{,} \PY{n}{atol} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}5}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sample test case passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Running sample test case
Sample test case passed

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} do not change code here}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{X\PYZus{}train}\PY{o}{=}\PY{n}{modify}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}test}\PY{o}{=}\PY{n}{modify}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    Plot the 1st two features and corresponding tumor type

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{}plot of the feature space}
         
         
         \PY{n}{c}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{;}
         \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{y\PYZus{}train}\PY{p}{:}    
             \PY{k}{if} \PY{n}{x}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{:}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{c}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{c}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{k}{if} \PY{n}{x}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{:}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{c}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{c}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{c}\PY{o}{=}\PY{n}{c}\PY{o}{+}\PY{l+m+mi}{1}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_12_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Task 2 : Sigmoid Function}\label{task-2-sigmoid-function}

Implement the sigmoid function in the function below. The sigmoid
function is given by \(\sigma(z) = 1/(1 + exp(-z))\)
\includegraphics{attachment:image.png}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{}define the sigmoid function (0.5 marks)}
         \PY{k}{def} \PY{n+nf}{sigmoid}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    Inputs:}
         \PY{l+s+sd}{        \PYZhy{}z : z can be either a number, a numpy 1d array or a numpy 2d array}
         \PY{l+s+sd}{    Output:}
         \PY{l+s+sd}{        sigmoid function applied to z, if z is a vector or a matrix the function must be applied to its each element}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{k}{return} \PY{l+m+mi}{1}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{*}\PY{n}{z}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} SAMPLE TEST CASES}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Running sample test case 1:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{sigmoid}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{n}{atol} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}5}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test case passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Running sample test case 2:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.73105858}\PY{p}{,} \PY{l+m+mf}{0.73105858}\PY{p}{,} \PY{l+m+mf}{0.73105858}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{atol} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}5}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test case passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Running sample test case 3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mf}{0.73105858}\PY{p}{,} \PY{l+m+mf}{0.88079708}\PY{p}{]}\PY{p}{,}
                \PY{p}{[}\PY{l+m+mf}{0.5}       \PY{p}{,} \PY{l+m+mf}{0.95257413}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{atol} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}5}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test case passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Running sample test case 1:
Test case passed
Running sample test case 2:
Test case passed
Running sample test case 3
Test case passed

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} do not change code here}
\end{Verbatim}


    \subsection{Task 3: Loss Function}\label{task-3-loss-function}

In order to obtain the appropriate values of weights (\(\theta_i\)) and
bias (\(b\)), we optimize a loss function which measures the deviation
of the model predictions from actual values. We use binary cross entropy
loss function (also called log loss) for logistic regression. It is
defined as:- \includegraphics{attachment:image.png} where y is the
actual value and p is the predicted value This loss function is obtained
by taking the negative log of the maximum likelihood of y. Implement the
log loss function below which takes as input the actual values and model
predictions and outputs the log loss.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{}define the log loss function (1 marks)}
         \PY{k}{def} \PY{n+nf}{log\PYZus{}loss}\PY{p}{(}\PY{n}{y}\PY{p}{,}\PY{n}{yhat}\PY{p}{)}\PY{p}{:} 
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    Inputs:}
         \PY{l+s+sd}{        \PYZhy{}y : The actual labels from the dataset. Shape [m,1]}
         \PY{l+s+sd}{        \PYZhy{}yhat : Model predictions. Shape [m,1]}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Output:}
         \PY{l+s+sd}{        A scalar value representing the log loss as defined above}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             \PY{k+kn}{import} \PY{n+nn}{math}
             \PY{k}{if} \PY{o+ow}{not} \PY{n}{np}\PY{o}{.}\PY{n}{isscalar}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{:}
             	\PY{n}{N} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}
             	\PY{n+nb}{sum} \PY{o}{=} \PY{l+m+mi}{0}
             	\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{N}\PY{p}{)}\PY{p}{:}
             		\PY{n+nb}{sum} \PY{o}{=} \PY{n+nb}{sum} \PY{o}{+} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{*}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{yhat}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{o}{+}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{yhat}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
             	\PY{k}{return} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n+nb}{sum}\PY{o}{/}\PY{n}{N}\PY{p}{)}
             \PY{k}{else}\PY{p}{:}
             	\PY{n+nb}{sum} \PY{o}{=} \PY{n}{y}\PY{o}{*}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{yhat}\PY{p}{)}\PY{o}{+}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{yhat}\PY{p}{)}
             	\PY{k}{return} \PY{o}{\PYZhy{}}\PY{n+nb}{sum}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} SAMPLE TEST CASE}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Running sample test case}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{log\PYZus{}loss}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,}\PY{l+m+mf}{0.6931471805599453}\PY{p}{,} \PY{n}{atol} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}5}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test case passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Running sample test case
Test case passed

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} do not change code here}
\end{Verbatim}


    \subsection{Task 4: Initialize weights and
bias.}\label{task-4-initialize-weights-and-bias.}

In the function below initialize the weights and bias as zeros

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{c+c1}{\PYZsh{}intialize the Weights and bias as zeros (0.5 marks)}
         \PY{k}{def} \PY{n+nf}{intialize}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    Return W and b with W being a column vector of size 2 containing 0 and b being a scalar value 0}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             \PY{n}{W} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{n}{b}\PY{o}{=}\PY{l+m+mi}{0}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{W}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{b}\PY{p}{)}
             \PY{k}{return} \PY{p}{[}\PY{n}{W}\PY{p}{,}\PY{n}{b}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} do not change code here}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{p}{[}\PY{n}{W}\PY{p}{,}\PY{n}{b}\PY{p}{]}\PY{o}{=}\PY{n}{intialize}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[[0.]
 [0.]]
0

    \end{Verbatim}

    \subsection{Task 5: Training the
classifier}\label{task-5-training-the-classifier}

Now that we have defined our loss function and initialized the weights
and bias we can start training our classifier. We train the classifier
by minimizing the loss function with respect to the weights and bias. To
carry out the optimization we use Gradient descent which works as:

Repeat till convergence: â€ƒ
\(\theta^{(i+1)} = \theta^{(i)} - \alpha\frac{\partial L}{\partial \theta}\)
where \(\alpha\) is the learning rate

The derivative of the log loss function with respect to weights and bias
are given as:

\(\frac{\partial L}{\partial \theta_{i}} = (\frac{1}{m})\sum_{j = 1}^{m}{(y'^{(j)} - y^{(j)})x_i^{(j)}}\)
\(\frac{\partial L}{\partial b} = (\frac{1}{m})\sum_{j = 1}^{m}{(y'^{(j)} - y^{(j)})}\)
\(where \; y'^{(j)} = \frac{1}{1 + e^{-(\theta^{T}x^{(j)})}}\)

Please note that the superscripts identifies the data point and the
subscript denotes the feature i.e. \(x_{1}^{(2)}\) will denote the first
feature of the second data point. \(y\) is the actual label and \(y'\)
is the value predicted by the model. Once we have the derivatives we can
use gradient descent to update the parameters.
\(\theta_{i} = \theta_{i} - \alpha\frac{\partial L}{\partial \theta_i}\)
\(b = b - \alpha\frac{\partial L}{\partial b}\)

Follow the following steps to implement the function:
\textsubscript{\textasciitilde{}} 1. Initialize weights and bias 2.
Repeat till convergence: 2.1 Compute the weighted sum with added bias
(w1x1 + w2x2 + b) for all the data points 2.2 Apply sigmooid function to
the resulting value to get model prediction 2.3 Compute the loss
function 2.4 Compute the derivative w.r.t the weights and bias using the
equations mentioned above 2.5 Using the derivatives update the weights
and bias using the gradient descent update rule 3. Return the final
weights and bias \textsubscript{\textasciitilde{}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Implement the update equations for gradient descent and run it for 10000 iterations.}
         \PY{l+s+sd}{   Append the loss values to losses list after every 100 iterations\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{c+c1}{\PYZsh{} (1.5 marks)}
         \PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{0.02}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    Inputs:}
         \PY{l+s+sd}{        \PYZhy{} X\PYZus{}train : Training data matrix. shape [m,n]}
         \PY{l+s+sd}{        \PYZhy{} y\PYZus{}train : Training labels. shape [m,1]}
         \PY{l+s+sd}{        \PYZhy{} lr : learning rate for the gradient descent algorithm}
         \PY{l+s+sd}{    Outputs:}
         \PY{l+s+sd}{        weights W (shape [2,1]) and bias b (scalar value)}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             \PY{n}{losses}\PY{o}{=}\PY{p}{[}\PY{p}{]}
             \PY{p}{[}\PY{n}{W}\PY{p}{,}\PY{n}{b}\PY{p}{]}\PY{o}{=}\PY{n}{intialize}\PY{p}{(}\PY{p}{)}
             \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10000}\PY{p}{)}\PY{p}{:}
                 \PY{n}{m}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
                 \PY{n}{z} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{W}\PY{p}{)}\PY{o}{+}\PY{n}{b}
                 \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{z}\PY{p}{)}
                 \PY{n}{loss} \PY{o}{=} \PY{n}{log\PYZus{}loss}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}
                 \PY{k}{if} \PY{n}{epoch}\PY{o}{\PYZpc{}}\PY{k}{100}==0:
                 	\PY{n}{losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
                 \PY{n}{diff} \PY{o}{=} \PY{n}{y\PYZus{}pred}\PY{o}{\PYZhy{}}\PY{n}{y\PYZus{}train}
                 \PY{n}{theta\PYZus{}1} \PY{o}{=} \PY{n}{lr}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{diff}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{p}{(}\PY{n}{m}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n}{m}
                 \PY{n}{theta\PYZus{}2} \PY{o}{=} \PY{n}{lr}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{diff}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{p}{(}\PY{n}{m}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n}{m}
                 \PY{n}{theta\PYZus{}b} \PY{o}{=} \PY{n}{lr}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{diff}\PY{p}{)}\PY{o}{/}\PY{n}{m}
                 \PY{n}{W}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{W}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{n}{theta\PYZus{}1}
                 \PY{n}{W}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{W}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{n}{theta\PYZus{}2}
                 \PY{n}{b} \PY{o}{=} \PY{n}{b}\PY{o}{\PYZhy{}}\PY{n}{theta\PYZus{}b}           
             \PY{k}{return} \PY{p}{[}\PY{n}{W}\PY{p}{,}\PY{n}{b}\PY{p}{,}\PY{n}{losses}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} SAMPLE TEST CASES}
         \PY{n}{W}\PY{p}{,}\PY{n}{b}\PY{p}{,}\PY{n}{a} \PY{o}{=} \PY{n}{train}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Running sample test case1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{a}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{l+m+mf}{0.6931471805599453}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test case passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Running sample test case 2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{W}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.4596111} \PY{p}{]}\PY{p}{,}
                \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.02750468}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test case passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[[0.]
 [0.]]
0
Running sample test case1
Test case passed
Running sample test case 2
Test case passed

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} do not change code here}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{c+c1}{\PYZsh{}lets look at the parameters obtained after training}
         \PY{p}{[}\PY{n}{W}\PY{p}{,}\PY{n}{b}\PY{p}{,}\PY{n}{a}\PY{p}{]}\PY{o}{=}\PY{n}{train}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{W}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{b}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{a}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[[0.]
 [0.]]
0
[[-0.4596111 ]
 [-0.02750468]]
7.4927179890376925

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}30}]:} [<matplotlib.lines.Line2D at 0x7eff87bc1160>]
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Task 6 : Predict}\label{task-6-predict}

Now that we have obtained suitable weights and biases we can use our
logistic regression model to give predictions on a given set of data.
Implement the predict function below which takes as input a matrix
containing the data points and returns the predictions for all the
points. Label all the points as 1 which have model outputs greater than
0.5 and 0 which have model outputs less than 0.5

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{c+c1}{\PYZsh{}calculate the labels for training examples and populate the list preds (0.5 marks)}
         \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{b}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    Inputs:}
         \PY{l+s+sd}{        \PYZhy{}X : Data matrix. Shape [m,n]}
         \PY{l+s+sd}{        \PYZhy{}W : Weights of logistic regression model}
         \PY{l+s+sd}{        \PYZhy{}b : bias of logistic regression model}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Output:}
         \PY{l+s+sd}{        predictions array of size m containing 0\PYZsq{}s or 1\PYZsq{}s representing negative and postive class respectively. }
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             \PY{n}{preds}\PY{o}{=}\PY{p}{[}\PY{p}{]}
             \PY{n}{X} \PY{o}{=} \PY{n}{modify}\PY{p}{(}\PY{n}{X}\PY{p}{)}
             \PY{n}{y\PYZus{}pred} \PY{o}{=}  \PY{n}{sigmoid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{W}\PY{p}{)}\PY{o}{+}\PY{n}{b}\PY{p}{)}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                 \PY{k}{if} \PY{n}{y\PYZus{}pred}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{\PYZgt{}}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{:}
                     \PY{n}{preds}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{k}{else}\PY{p}{:}
                     \PY{n}{preds}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
             \PY{k}{return} \PY{n}{preds}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} SAMPLE TEST CASE}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Running sample test case 1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{preds}\PY{o}{=}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{b}\PY{p}{)}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{preds}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test case passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Running sample test case 2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{preds}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test case passed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Running sample test case 1
Test case passed
Running sample test case 2
Test case passed

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} do not change code here}
\end{Verbatim}


    \subsection{Task 7 : Find Accuracy (0.25
marks)}\label{task-7-find-accuracy-0.25-marks}

Your final task for this assignment is to compute the accuracy of our
classifier. Implement the find\_accuracy function which takes as the
input the predictions of our classifier and the actual labels and
computes the accuracy. We define accuracy as the ratio between number of
correctly classified examples and total number of examples. For eg. if a
classifier classifies 20 out of 100 examples correctly then the accuracy
will be 20\%. You should expect 89\% accuracy on the training set and
87\% accuracy on the test set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{k}{def} \PY{n+nf}{find\PYZus{}accuracy}\PY{p}{(}\PY{n}{y\PYZus{}preds}\PY{p}{,} \PY{n}{y\PYZus{}true}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    Calculates the accuracy of the classifier.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Inputs:}
         \PY{l+s+sd}{        \PYZhy{}y\PYZus{}preds : Predictions by KNN Classifier}
         \PY{l+s+sd}{        \PYZhy{}y\PYZus{}true : Actual labels}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{    Output:}
         \PY{l+s+sd}{        Accuracy in percentage which is defined as : 100*number\PYZus{}of\PYZus{}correctly\PYZus{}classified\PYZus{}examples/total\PYZus{}examples}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             
             \PY{n}{acc} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}preds}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{k}{if} \PY{n}{y\PYZus{}true}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{==}\PY{n}{y\PYZus{}preds}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{:}
                     \PY{n}{acc} \PY{o}{=} \PY{n}{acc}\PY{o}{+}\PY{l+m+mi}{1}
             \PY{n}{acc} \PY{o}{=} \PY{n}{acc}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}preds}\PY{p}{)}
             \PY{k}{return} \PY{n}{acc}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n}{preds\PYZus{}train} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{b}\PY{p}{)}
         \PY{n}{preds\PYZus{}test} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{b}\PY{p}{)}
         \PY{n}{acc\PYZus{}train} \PY{o}{=} \PY{n}{find\PYZus{}accuracy}\PY{p}{(}\PY{n}{preds\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{acc\PYZus{}test} \PY{o}{=} \PY{n}{find\PYZus{}accuracy}\PY{p}{(}\PY{n}{preds\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{acc\PYZus{}train}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{acc\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
0.8873239436619719
0.8881118881118881

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} HIDDEN TEST CASE}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} BEGIN TEST}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{acc\PYZus{}train}\PY{p}{,} \PY{l+m+mf}{0.8873239436619719}\PY{p}{,} \PY{n}{atol} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{acc\PYZus{}test}\PY{p}{,} \PY{l+m+mf}{0.8881118881118881}\PY{p}{,} \PY{n}{atol} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END TEST}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{c+c1}{\PYZsh{}the following code prints the decision boundry for our problem on the training dataset}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{c}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{ax}\PY{o}{=}\PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
         \PY{n}{xvals}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{ax}\PY{o}{.}\PY{n}{get\PYZus{}xlim}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{yvals}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{xvals}\PY{o}{*}\PY{n}{W}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{+}\PY{n}{b}\PY{p}{)}\PY{o}{/}\PY{n}{W}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{xvals}\PY{p}{,}\PY{n}{yvals}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{40}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}44}]:} (0, 40)
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_38_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Summary}\label{summary}

We have implemented a simple logistic regression in this notebook. We
have taken two features so that it is easy to visualize this problem. We
can use additional features while training and you can try implementing
this algorithm with more number of features.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
